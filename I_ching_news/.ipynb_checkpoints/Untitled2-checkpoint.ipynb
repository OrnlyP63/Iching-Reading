{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, ...,  9,  4,  9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({'content': dataset.data, 'topic': dataset.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  topic\n",
       "0      Well i'm not sure about the story nad it did s...     17\n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...      0\n",
       "2      Although I realize that principle is not one o...     17\n",
       "3      Notwithstanding all the legitimate fuss about ...     11\n",
       "4      Well, I will have to change the scoring on my ...     10\n",
       "...                                                  ...    ...\n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...     17\n",
       "11310                                                 \\n     13\n",
       "11311  \\nI agree.  Home runs off Clemens are always m...      9\n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...      4\n",
       "11313                                        ^^^^^^\\n...      9\n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=re.sub('[^a-zA-z]',' ',documents['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well i m not sure about the story nad it did seem biased  What I disagree with is your statement that the U S  Media is out to ruin Israels reputation  That is rediculous  The U S  media is the most pro israeli media in the world  Having lived in Europe I realize that incidences such as the one described in the letter have occured  The U S  media as a whole seem to try to ignore them  The U S  is subsidizing Israels existance and the Europeans are not  at least not to the same degree   So I think that might be a reason they report more clearly on the atrocities   What is a shame is that in Austria  daily reports of the inhuman acts commited by Israeli soldiers and the blessing received from the Government makes some of the Holocaust guilt go away  After all  look how the Jews are treating other races when they got power  It is unfortunate  '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well i m not sure about the story nad it did seem biased  what i disagree with is your statement that the u s  media is out to ruin israels reputation  that is rediculous  the u s  media is the most pro israeli media in the world  having lived in europe i realize that incidences such as the one described in the letter have occured  the u s  media as a whole seem to try to ignore them  the u s  is subsidizing israels existance and the europeans are not  at least not to the same degree   so i think that might be a reason they report more clearly on the atrocities   what is a shame is that in austria  daily reports of the inhuman acts commited by israeli soldiers and the blessing received from the government makes some of the holocaust guilt go away  after all  look how the jews are treating other races when they got power  it is unfortunate  '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'i',\n",
       " 'm',\n",
       " 'not',\n",
       " 'sure',\n",
       " 'about',\n",
       " 'the',\n",
       " 'story',\n",
       " 'nad',\n",
       " 'it',\n",
       " 'did',\n",
       " 'seem',\n",
       " 'biased',\n",
       " 'what',\n",
       " 'i',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'is',\n",
       " 'your',\n",
       " 'statement',\n",
       " 'that',\n",
       " 'the',\n",
       " 'u',\n",
       " 's',\n",
       " 'media',\n",
       " 'is',\n",
       " 'out',\n",
       " 'to',\n",
       " 'ruin',\n",
       " 'israels',\n",
       " 'reputation',\n",
       " 'that',\n",
       " 'is',\n",
       " 'rediculous',\n",
       " 'the',\n",
       " 'u',\n",
       " 's',\n",
       " 'media',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'pro',\n",
       " 'israeli',\n",
       " 'media',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'having',\n",
       " 'lived',\n",
       " 'in',\n",
       " 'europe',\n",
       " 'i',\n",
       " 'realize',\n",
       " 'that',\n",
       " 'incidences',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'one',\n",
       " 'described',\n",
       " 'in',\n",
       " 'the',\n",
       " 'letter',\n",
       " 'have',\n",
       " 'occured',\n",
       " 'the',\n",
       " 'u',\n",
       " 's',\n",
       " 'media',\n",
       " 'as',\n",
       " 'a',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'try',\n",
       " 'to',\n",
       " 'ignore',\n",
       " 'them',\n",
       " 'the',\n",
       " 'u',\n",
       " 's',\n",
       " 'is',\n",
       " 'subsidizing',\n",
       " 'israels',\n",
       " 'existance',\n",
       " 'and',\n",
       " 'the',\n",
       " 'europeans',\n",
       " 'are',\n",
       " 'not',\n",
       " 'at',\n",
       " 'least',\n",
       " 'not',\n",
       " 'to',\n",
       " 'the',\n",
       " 'same',\n",
       " 'degree',\n",
       " 'so',\n",
       " 'i',\n",
       " 'think',\n",
       " 'that',\n",
       " 'might',\n",
       " 'be',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'they',\n",
       " 'report',\n",
       " 'more',\n",
       " 'clearly',\n",
       " 'on',\n",
       " 'the',\n",
       " 'atrocities',\n",
       " 'what',\n",
       " 'is',\n",
       " 'a',\n",
       " 'shame',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'austria',\n",
       " 'daily',\n",
       " 'reports',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inhuman',\n",
       " 'acts',\n",
       " 'commited',\n",
       " 'by',\n",
       " 'israeli',\n",
       " 'soldiers',\n",
       " 'and',\n",
       " 'the',\n",
       " 'blessing',\n",
       " 'received',\n",
       " 'from',\n",
       " 'the',\n",
       " 'government',\n",
       " 'makes',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'go',\n",
       " 'away',\n",
       " 'after',\n",
       " 'all',\n",
       " 'look',\n",
       " 'how',\n",
       " 'the',\n",
       " 'jews',\n",
       " 'are',\n",
       " 'treating',\n",
       " 'other',\n",
       " 'races',\n",
       " 'when',\n",
       " 'they',\n",
       " 'got',\n",
       " 'power',\n",
       " 'it',\n",
       " 'is',\n",
       " 'unfortunate']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[PorterStemmer().stem(word) for word in text if not word in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'sure',\n",
       " 'stori',\n",
       " 'nad',\n",
       " 'seem',\n",
       " 'bias',\n",
       " 'disagre',\n",
       " 'statement',\n",
       " 'u',\n",
       " 'media',\n",
       " 'ruin',\n",
       " 'israel',\n",
       " 'reput',\n",
       " 'redicul',\n",
       " 'u',\n",
       " 'media',\n",
       " 'pro',\n",
       " 'isra',\n",
       " 'media',\n",
       " 'world',\n",
       " 'live',\n",
       " 'europ',\n",
       " 'realiz',\n",
       " 'incid',\n",
       " 'one',\n",
       " 'describ',\n",
       " 'letter',\n",
       " 'occur',\n",
       " 'u',\n",
       " 'media',\n",
       " 'whole',\n",
       " 'seem',\n",
       " 'tri',\n",
       " 'ignor',\n",
       " 'u',\n",
       " 'subsid',\n",
       " 'israel',\n",
       " 'exist',\n",
       " 'european',\n",
       " 'least',\n",
       " 'degre',\n",
       " 'think',\n",
       " 'might',\n",
       " 'reason',\n",
       " 'report',\n",
       " 'clearli',\n",
       " 'atroc',\n",
       " 'shame',\n",
       " 'austria',\n",
       " 'daili',\n",
       " 'report',\n",
       " 'inhuman',\n",
       " 'act',\n",
       " 'commit',\n",
       " 'isra',\n",
       " 'soldier',\n",
       " 'bless',\n",
       " 'receiv',\n",
       " 'govern',\n",
       " 'make',\n",
       " 'holocaust',\n",
       " 'guilt',\n",
       " 'go',\n",
       " 'away',\n",
       " 'look',\n",
       " 'jew',\n",
       " 'treat',\n",
       " 'race',\n",
       " 'got',\n",
       " 'power',\n",
       " 'unfortun']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for doc in documents.content:\n",
    "    text=re.sub('[^a-zA-z]',' ', doc)\n",
    "    text=text.lower()\n",
    "    text=text.split()\n",
    "    text=[PorterStemmer().stem(word) for word in text if not word in stop]\n",
    "    text=' '.join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['clean_content'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>17</td>\n",
       "      <td>well sure stori nad seem bias disagre statemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah expect peopl read faq etc actual accept h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "      <td>17</td>\n",
       "      <td>although realiz principl one strongest point w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "      <td>11</td>\n",
       "      <td>notwithstand legitim fuss propos much chang at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "      <td>10</td>\n",
       "      <td>well chang score playoff pool unfortun time ri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  topic  \\\n",
       "0  Well i'm not sure about the story nad it did s...     17   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...      0   \n",
       "2  Although I realize that principle is not one o...     17   \n",
       "3  Notwithstanding all the legitimate fuss about ...     11   \n",
       "4  Well, I will have to change the scoring on my ...     10   \n",
       "\n",
       "                                       clean_content  \n",
       "0  well sure stori nad seem bias disagre statemen...  \n",
       "1  yeah expect peopl read faq etc actual accept h...  \n",
       "2  although realiz principl one strongest point w...  \n",
       "3  notwithstand legitim fuss propos much chang at...  \n",
       "4  well chang score playoff pool unfortun time ri...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents.clean_content.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, stori, nad, seem, bias, disagre, ...\n",
       "1        [yeah, expect, peopl, read, faq, etc, actual, ...\n",
       "2        [although, realiz, principl, one, strongest, p...\n",
       "3        [notwithstand, legitim, fuss, propos, much, ch...\n",
       "4        [well, chang, score, playoff, pool, unfortun, ...\n",
       "                               ...                        \n",
       "11309    [danni, rubenstein, isra, journalist, speak, t...\n",
       "11310                                                   []\n",
       "11311    [agre, home, run, clemen, alway, memor, kinda,...\n",
       "11312    [use, hp, deskjet, orang, micro, grappler, ls,...\n",
       "11313    [^^^^^^, argument, murphi, scare, hell, came, ...\n",
       "Name: clean_content, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 act\n",
      "1 atroc\n",
      "2 austria\n",
      "3 away\n",
      "4 bias\n",
      "5 bless\n",
      "6 clearli\n",
      "7 commit\n",
      "8 daili\n",
      "9 degre\n",
      "10 describ\n",
      "11 disagre\n",
      "12 europ\n",
      "13 european\n",
      "14 exist\n",
      "15 go\n",
      "16 got\n",
      "17 govern\n",
      "18 guilt\n",
      "19 holocaust\n",
      "20 ignor\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count+=1\n",
    "    if count>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15,no_above=0.1,keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awar'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus=[dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 1),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (81, 1),\n",
       " (147, 1),\n",
       " (187, 2),\n",
       " (224, 1),\n",
       " (252, 1),\n",
       " (255, 1),\n",
       " (269, 1),\n",
       " (283, 1),\n",
       " (293, 1),\n",
       " (300, 3),\n",
       " (305, 2),\n",
       " (319, 1),\n",
       " (330, 1),\n",
       " (353, 1),\n",
       " (404, 3),\n",
       " (422, 2),\n",
       " (426, 2),\n",
       " (429, 1),\n",
       " (453, 2),\n",
       " (491, 1),\n",
       " (529, 1),\n",
       " (531, 1),\n",
       " (573, 1),\n",
       " (576, 1),\n",
       " (578, 1),\n",
       " (604, 1),\n",
       " (622, 1),\n",
       " (632, 2),\n",
       " (687, 2),\n",
       " (692, 1),\n",
       " (840, 1),\n",
       " (845, 1),\n",
       " (857, 2),\n",
       " (913, 1),\n",
       " (928, 1),\n",
       " (929, 1),\n",
       " (930, 1),\n",
       " (931, 1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 12 (\"least\") appears 1 time.\n",
      "Word 61 (\"center\") appears 1 time.\n",
      "Word 62 (\"continu\") appears 1 time.\n",
      "Word 81 (\"attempt\") appears 1 time.\n",
      "Word 147 (\"complet\") appears 1 time.\n",
      "Word 187 (\"without\") appears 2 time.\n",
      "Word 224 (\"howev\") appears 1 time.\n",
      "Word 252 (\"ago\") appears 1 time.\n",
      "Word 255 (\"along\") appears 1 time.\n",
      "Word 269 (\"articl\") appears 1 time.\n",
      "Word 283 (\"build\") appears 1 time.\n",
      "Word 293 (\"co\") appears 1 time.\n",
      "Word 300 (\"commerci\") appears 3 time.\n",
      "Word 305 (\"connect\") appears 2 time.\n",
      "Word 319 (\"data\") appears 1 time.\n",
      "Word 330 (\"develop\") appears 1 time.\n",
      "Word 353 (\"exampl\") appears 1 time.\n",
      "Word 404 (\"internet\") appears 3 time.\n",
      "Word 422 (\"long\") appears 2 time.\n",
      "Word 426 (\"machin\") appears 2 time.\n",
      "Word 429 (\"major\") appears 1 time.\n",
      "Word 453 (\"network\") appears 2 time.\n",
      "Word 491 (\"provid\") appears 1 time.\n",
      "Word 529 (\"sinc\") appears 1 time.\n",
      "Word 531 (\"softwar\") appears 1 time.\n",
      "Word 573 (\"univers\") appears 1 time.\n",
      "Word 576 (\"upgrad\") appears 1 time.\n",
      "Word 578 (\"usa\") appears 1 time.\n",
      "Word 604 (\"realli\") appears 1 time.\n",
      "Word 622 (\"appropri\") appears 1 time.\n",
      "Word 632 (\"privat\") appears 2 time.\n",
      "Word 687 (\"restrict\") appears 2 time.\n",
      "Word 692 (\"nation\") appears 1 time.\n",
      "Word 840 (\"cut\") appears 1 time.\n",
      "Word 845 (\"involv\") appears 1 time.\n",
      "Word 857 (\"hit\") appears 2 time.\n",
      "Word 913 (\"success\") appears 1 time.\n",
      "Word 928 (\"feder\") appears 1 time.\n",
      "Word 929 (\"five\") appears 1 time.\n",
      "Word 930 (\"intent\") appears 1 time.\n",
      "Word 931 (\"opinion\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_corpus_100=bow_corpus[100]\n",
    "for i in range(len(bow_corpus_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_corpus_100[i][0], \n",
    "                                                     dictionary[bow_corpus_100[i][0]], \n",
    "                                                     bow_corpus_100[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF on our document set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora,models\n",
    "tfidf=models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf=tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12, 0.08253815092460594),\n",
       " (61, 0.11389561343793152),\n",
       " (62, 0.10284838387398672),\n",
       " (81, 0.11223398939700842),\n",
       " (147, 0.09600151826150971),\n",
       " (187, 0.15667410081201583),\n",
       " (224, 0.08045970396259487),\n",
       " (252, 0.09543795656816574),\n",
       " (255, 0.11355594340310489),\n",
       " (269, 0.09494563565706723),\n",
       " (283, 0.10292765419751533),\n",
       " (293, 0.12946276642774487),\n",
       " (300, 0.3912837950324558),\n",
       " (305, 0.21773947865775956),\n",
       " (319, 0.09722671670367669),\n",
       " (330, 0.10048823866877195),\n",
       " (353, 0.08968925004262342),\n",
       " (404, 0.36144182847034434),\n",
       " (422, 0.16091940792518974),\n",
       " (426, 0.20156534686758842),\n",
       " (429, 0.09749106751591627),\n",
       " (453, 0.23130667434547908),\n",
       " (491, 0.09100093546161059),\n",
       " (529, 0.07127196766358884),\n",
       " (531, 0.09749106751591627),\n",
       " (573, 0.09702995127986737),\n",
       " (576, 0.13023243163492645),\n",
       " (578, 0.13142474727037406),\n",
       " (604, 0.07158174493445366),\n",
       " (622, 0.125213982156965),\n",
       " (632, 0.24326671997577892),\n",
       " (687, 0.255959076056608),\n",
       " (692, 0.09755751596171205),\n",
       " (840, 0.11223398939700842),\n",
       " (845, 0.10532130366556165),\n",
       " (857, 0.2246839658904127),\n",
       " (913, 0.12119594018444427),\n",
       " (928, 0.12588214425866937),\n",
       " (929, 0.1265653224890307),\n",
       " (930, 0.13203891264056788),\n",
       " (931, 0.10229920490596278)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=gensim.models.LdaMulticore(bow_corpus,\n",
    "                                    num_topics=10,\n",
    "                                    id2word=dictionary,\n",
    "                                    passes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.012*\"window\" + 0.008*\"system\" + 0.008*\"file\" + 0.008*\"w\" + 0.008*\"program\" + 0.006*\"do\" + 0.006*\"]\" + 0.006*\"help\" + 0.006*\"key\" + 0.005*\"chip\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.007*\"god\" + 0.007*\"believ\" + 0.006*\"power\" + 0.005*\"us\" + 0.005*\"mean\" + 0.004*\"system\" + 0.004*\"case\" + 0.004*\"realli\" + 0.004*\"differ\" + 0.004*\"question\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.008*\"x\" + 0.006*\"group\" + 0.006*\"israel\" + 0.005*\"card\" + 0.005*\"us\" + 0.005*\"state\" + 0.005*\"isra\" + 0.004*\"war\" + 0.004*\"number\" + 0.004*\"set\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.023*\"f\" + 0.018*\"u\" + 0.017*\"g\" + 0.016*\"v\" + 0.015*\"b\" + 0.014*\"r\" + 0.012*\"l\" + 0.012*\"]\" + 0.011*\"p\" + 0.010*\"w\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.008*\"law\" + 0.006*\"imag\" + 0.006*\"gun\" + 0.006*\"drive\" + 0.005*\"control\" + 0.005*\"file\" + 0.005*\"u\" + 0.005*\"case\" + 0.004*\"christian\" + 0.004*\"e\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.033*\"w\" + 0.024*\"x\" + 0.016*\"b\" + 0.011*\"v\" + 0.011*\"c\" + 0.010*\"drive\" + 0.009*\"r\" + 0.009*\"k\" + 0.008*\"]\" + 0.007*\"scsi\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.074*\"x\" + 0.046*\"g\" + 0.044*\"f\" + 0.042*\"r\" + 0.036*\"u\" + 0.033*\"w\" + 0.032*\"b\" + 0.030*\"v\" + 0.025*\"p\" + 0.025*\"e\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.009*\"game\" + 0.008*\"said\" + 0.008*\"f\" + 0.007*\"us\" + 0.007*\"play\" + 0.007*\"team\" + 0.006*\"day\" + 0.005*\"back\" + 0.004*\"run\" + 0.004*\"last\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.009*\"p\" + 0.008*\"edu\" + 0.008*\"com\" + 0.008*\"c\" + 0.007*\"v\" + 0.007*\"file\" + 0.006*\"g\" + 0.005*\"book\" + 0.005*\"x\" + 0.005*\"read\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.011*\"god\" + 0.011*\"system\" + 0.008*\"space\" + 0.006*\"question\" + 0.006*\"inform\" + 0.005*\"govern\" + 0.005*\"exist\" + 0.005*\"state\" + 0.005*\"x\" + 0.004*\"answer\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10, \n",
    "                                             id2word = dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.009*\"drive\" + 0.007*\"scsi\" + 0.006*\"believ\" + 0.005*\"christian\" + 0.005*\"k\" + 0.004*\"e\" + 0.004*\"god\" + 0.004*\"mail\" + 0.004*\"exist\" + 0.004*\"b\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.005*\"law\" + 0.005*\"driver\" + 0.004*\"window\" + 0.004*\"car\" + 0.004*\"com\" + 0.004*\"ground\" + 0.004*\"pictur\" + 0.004*\"x\" + 0.003*\"state\" + 0.003*\"system\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.006*\"drive\" + 0.005*\"mb\" + 0.005*\"x\" + 0.005*\"sound\" + 0.004*\"window\" + 0.004*\"christian\" + 0.004*\"disk\" + 0.004*\"god\" + 0.004*\"control\" + 0.004*\"system\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.006*\"x\" + 0.006*\"mail\" + 0.005*\"system\" + 0.005*\"com\" + 0.004*\"number\" + 0.004*\"e\" + 0.004*\"edu\" + 0.004*\"someon\" + 0.004*\"send\" + 0.004*\"v\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.010*\"game\" + 0.008*\"team\" + 0.007*\"play\" + 0.007*\"player\" + 0.006*\"car\" + 0.005*\"leagu\" + 0.004*\"lot\" + 0.004*\"last\" + 0.004*\"win\" + 0.004*\"realli\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.006*\"n\" + 0.006*\"israel\" + 0.005*\"edu\" + 0.005*\"]\" + 0.005*\"jew\" + 0.004*\"kill\" + 0.004*\"soon\" + 0.004*\"bank\" + 0.004*\"day\" + 0.004*\"[\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.012*\"window\" + 0.010*\"file\" + 0.008*\"x\" + 0.007*\"c\" + 0.007*\"program\" + 0.006*\"do\" + 0.006*\"run\" + 0.005*\"card\" + 0.005*\"edu\" + 0.005*\"graphic\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.007*\"god\" + 0.005*\"jesu\" + 0.004*\"key\" + 0.004*\"us\" + 0.004*\"christian\" + 0.004*\"someon\" + 0.004*\"question\" + 0.003*\"said\" + 0.003*\"person\" + 0.003*\"believ\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.006*\"bike\" + 0.005*\"help\" + 0.005*\"x\" + 0.004*\"address\" + 0.004*\"win\" + 0.004*\"appreci\" + 0.004*\"anybodi\" + 0.004*\"email\" + 0.004*\"error\" + 0.004*\"bu\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.006*\"game\" + 0.004*\"engin\" + 0.004*\"run\" + 0.004*\"power\" + 0.004*\"system\" + 0.004*\"mhz\" + 0.004*\"mail\" + 0.004*\"test\" + 0.004*\"edu\" + 0.004*\"chip\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intent',\n",
       " 'nren',\n",
       " 'spectat',\n",
       " 'nren',\n",
       " 'year',\n",
       " 'commerci',\n",
       " 'ip',\n",
       " 'softwar',\n",
       " 'vendor',\n",
       " 'realli',\n",
       " 'profession',\n",
       " 'opinion',\n",
       " 'nren',\n",
       " 'point',\n",
       " 'irrelev',\n",
       " 'privat',\n",
       " 'sector',\n",
       " 'network',\n",
       " 'deploy',\n",
       " 'five',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'would',\n",
       " 'major',\n",
       " 'develop',\n",
       " 'howev',\n",
       " 'upgrad',\n",
       " 'nsfnet',\n",
       " 'attempt',\n",
       " 'reviv',\n",
       " 'lag',\n",
       " 'use',\n",
       " 'nation',\n",
       " 'supercomput',\n",
       " 'center',\n",
       " 'could',\n",
       " 'cut',\n",
       " 'nsfnet',\n",
       " 'complet',\n",
       " 'internet',\n",
       " 'would',\n",
       " 'continu',\n",
       " 'chug',\n",
       " 'along',\n",
       " 'without',\n",
       " 'hiccup',\n",
       " 'asid',\n",
       " 'univers',\n",
       " 'long',\n",
       " 'haul',\n",
       " 'network',\n",
       " 'internet',\n",
       " 'connect',\n",
       " 'long',\n",
       " 'sinc',\n",
       " 'ceas',\n",
       " 'feder',\n",
       " 'sponsorship',\n",
       " 'regul',\n",
       " 'least',\n",
       " 'usa',\n",
       " 'success',\n",
       " 'cix',\n",
       " 'commerci',\n",
       " 'internet',\n",
       " 'exchang',\n",
       " 'prime',\n",
       " 'exampl',\n",
       " 'dear',\n",
       " 'vp',\n",
       " 'promot',\n",
       " 'data',\n",
       " 'superhighway',\n",
       " 'privat',\n",
       " 'sector',\n",
       " 'build',\n",
       " 'without',\n",
       " 'nsfnet',\n",
       " 'restrict',\n",
       " 'illustr',\n",
       " 'connect',\n",
       " 'machin',\n",
       " 'desk',\n",
       " 'machin',\n",
       " 'articl',\n",
       " 'post',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'co',\n",
       " 'uk',\n",
       " 'involv',\n",
       " 'commerci',\n",
       " 'ip',\n",
       " 'provid',\n",
       " 'hit',\n",
       " 'amsterdam',\n",
       " 'nsfnet',\n",
       " 'nren',\n",
       " 'appropri',\n",
       " 'use',\n",
       " 'restrict',\n",
       " 'even',\n",
       " 'mbp',\n",
       " 'hit',\n",
       " 'eunet',\n",
       " 'gateway',\n",
       " 'qed']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9833267331123352\t \n",
      "Topic: 0.011*\"god\" + 0.011*\"system\" + 0.008*\"space\" + 0.006*\"question\" + 0.006*\"inform\" + 0.005*\"govern\" + 0.005*\"exist\" + 0.005*\"state\" + 0.005*\"x\" + 0.004*\"answer\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[100]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.98332673)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[bow_corpus[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.46135744), (7, 0.48147696)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[bow_corpus[4110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.26315895), (7, 0.6796723)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf[bow_corpus[4110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
